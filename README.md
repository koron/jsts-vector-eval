<https://github.com/yahoojapan/JGLUE>

## Memo

### OpenAIのembeddingsは不安定

OpenAIのembeddingsは、同一の文に対して微妙に異なるベクトルになっていることがある。
Googleのでは、そんなことはない。同一の文に対しては同じベクトルになっている。

ID:20は前半後半共に `室内の机の上にノートパソコンが置かれています。` で、どちらに対しても同じベクトルになっている。
ID:21の後半も同じ文なのに、若干異なるベクトルになっている。
以下にはID:20のものとID:21のものの冒頭7次元を抜粋したもの。

```
[-0.0015773575, -0.012670631, 0.003507396,  -0.0019979863, 0.0049698898, 0.01683809,  -0.024422348,
[-0.0013762178, -0.01236897,  0.0034336674, -0.0020307505, 0.0049417624, 0.016893255, -0.024479039,
```

今回はFaissのインデックスの性質を比較する目的であるため、このベクトルの不安定さはそぐわない。
よってOpenAIのほうはいったん忘れることにする。

### faiss.IndexFlatL2 の距離は平方根を取ってない

faiss.IndexFlatL2 の返す距離(L2ノルム)は二乗したもの(平方根を取ってないもの)だった。

### ~~Goで何か間違ってそう~~ (修正済)

Go版とPython版でEuclidの返す順序要素が微妙に違ってる。
`1_2` についての10番目がGoでは `127_1:0.498991` なのに対し、
Python では `1385_2:0.496871` になってる。
Python版が正しくGo版が間違ってる可能性が高い。

Go版に入れた微妙な最適化のせいだった。
当該最適化をキャンセルするように修正済み。

### PQ, OPQ, RQ(AQ) の所感

PQはどうしても量子化歪みが発生する。
順序の入れ替えが起る。
Mを増やせば遅くなる。
nを増やすとtraining pointsを多く必要とするようになる。

OPQはトレーニングに長い時間を要する。

RQのトレーニングはOPQよりも速いが、検索により時間がかかっているらしい。
Mを増やせば精度があがる。
nbitsの効果はよくわからない。

### Quantization Distortionを計測

k-NNの第1位が検索に利用したベクトル自身であることを利用して、
自身との距離を見ることで量子化歪み(QD)を計測する。

L2では歪み0で自分自身になる。

PQでは歪みが0.4~0.5くらいになり、稀に自身が第1位ではなくなる。
またnbitsを増やすと歪みが小さくなり、自身が第1位ではない確率が減る。

OPQでは歪みは 0.3 おおよそ未満になり、ほぼ自身が第1位になる。

RQでは0.1くらい歪み、自分自信が第1位になる。

このあたりをうまく可視化できれば。

### PQ + Inner Product

精度劣化がひどい。
自信が第1位ではなくなる確率が跳ね上がる。
PQのベクトル分割とコサイン距離空間との相性がわるいのではないか。
分割後のベクトルをコサイン距離空間で最適に量子化した場合、
それを合成して復元されたベクトルはコサイン距離空間では
差異的にならない構造がありそう。
ユークリッド空間ではないから。

### LSQ, RQの優位性について

次元dを大きくした際に(≒圧縮率が高くなった際に)LSQやRQが有利になるのではとの仮説を検証した。
結果は否で、次元を大きくしてもLSQやRQが有利になることはなかった。
しかしその検証過程で、学習に利用した既知のベクトルに対する量子化誤差がLSQとRQにおいて小さいことを確認した。
詳細は [./Qerror_d.ipynb](./Qerror_d.ipynb) を参照。

### Recall, Accuracy, Precision の定義

*   <https://www.kaggle.com/code/mahmoudreda55/knn-precision-and-recall>
*   [レコメンド指標の整理](https://zenn.dev/hellorusk/articles/7e336fd3c6be20a8f8d1)
